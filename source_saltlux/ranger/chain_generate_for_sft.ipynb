{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _init import *\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random, json\n",
    "from typing import List\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from ranger.utils import common_utils, json_utils, tokenizer_utils, file_utils, container_utils\n",
    "\n",
    "from ranger.corag.corag_result import QueryResult, ChainResult\n",
    "from ranger.chain_generate.chain_generate_client import request_chain_generate\n",
    "from ranger.reward.reward_calculator import RewardCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dad736",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "common_utils.set_seed(seed)\n",
    "\n",
    "\n",
    "def datas_shuffle(datas: list, seed: int):\n",
    "    rng = random.Random(seed)\n",
    "    rng.shuffle(datas)\n",
    "\n",
    "\n",
    "def load_datas(train_data_path: str, test_data_path: str, seed=-1):\n",
    "    train_datas = json_utils.load_file(train_data_path)\n",
    "    test_datas = json_utils.load_file(test_data_path)\n",
    "\n",
    "    if seed != -1:\n",
    "        datas_shuffle(train_datas, seed)\n",
    "        datas_shuffle(test_datas, seed)\n",
    "    \n",
    "    return train_datas, test_datas\n",
    "\n",
    "\n",
    "work_dir = f'/home/nlpshlee/dev_env/git/repos/ranger'\n",
    "data_dir = f'{work_dir}/data'\n",
    "out_dir = f'{work_dir}/data/sft'\n",
    "\n",
    "train_data_path = f'{data_dir}/custom_musique_train_5000_final.jsonl'\n",
    "test_data_path = f'{data_dir}/custom_multihopqa_eval_1000.jsonl'\n",
    "train_datas, test_datas = load_datas(train_data_path, test_data_path)\n",
    "\n",
    "tokenizer = tokenizer_utils.load_tokenizer(VLLM_CONFIG['model_name'])\n",
    "bos_token_id = tokenizer.bos_token_id\n",
    "\n",
    "reward_calculator = RewardCalculator(REWARD_CONFIG['reward_option'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96838fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_max_new_tokens(answer: str, tokenizer: PreTrainedTokenizerFast):\n",
    "#     token_ids = tokenizer(answer, add_special_tokens=False)['input_ids']\n",
    "#     # print(f'token_ids len : {len(token_ids)}')\n",
    "\n",
    "#     start_idx = 1 if token_ids[0] == bos_token_id else 0\n",
    "#     cur_len = -1\n",
    "\n",
    "#     for i in range(start_idx, len(token_ids)):\n",
    "#         decoded = tokenizer.decode(token_ids[start_idx:i+1])\n",
    "\n",
    "#         if answer == decoded:\n",
    "#             cur_len = (i+1)-start_idx\n",
    "#             # print(f'{start_idx} - {i+1}')\n",
    "#             # print(decoded)\n",
    "#             # print(f'cur_len : {cur_len}')\n",
    "#             break\n",
    "    \n",
    "#     return cur_len\n",
    "\n",
    "\n",
    "# datas = train_datas + test_datas\n",
    "# print(f'datas size : {len(datas)}')\n",
    "\n",
    "# max_len = -1\n",
    "\n",
    "# for data in datas:\n",
    "#     answers = data['answers']\n",
    "\n",
    "#     for answer in answers:\n",
    "#         # print(f'answer : {answer}')\n",
    "#         cur_len = calculate_max_new_tokens(answer, tokenizer)\n",
    "\n",
    "#         if max_len < cur_len:\n",
    "#             max_len = cur_len\n",
    "\n",
    "# print(f'max_len : {max_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b76956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_responses_dict = []\n",
    "\n",
    "# for i, data in enumerate(train_datas):\n",
    "#     print(f'[{i}] query : {data[\"query\"]}')\n",
    "\n",
    "#     responses: List[QueryResult] = request_chain_generate([data], 1, 5, 5)\n",
    "#     responses_dict = [query_result.to_dict() for query_result in responses]\n",
    "#     all_responses_dict.extend(responses_dict)\n",
    "\n",
    "#     if i == 4:\n",
    "#         break\n",
    "\n",
    "# responses: List[QueryResult] = request_chain_generate(train_datas[:5], 5, 5, 5)\n",
    "# responses_dict = [query_result.to_dict() for query_result in responses]\n",
    "# all_responses_dict.extend(responses_dict)\n",
    "\n",
    "# print(f'{json_utils.to_str(all_responses_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bdc02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_generate_for_sft(datas, batch_size, n_chains, chain_depth, out_dir):\n",
    "    for datas_batch in container_utils.chunks(datas, batch_size):\n",
    "        query_results: List[QueryResult] = request_chain_generate(datas_batch, batch_size, n_chains, chain_depth)\n",
    "\n",
    "        reward_calculator.calculate_reward_and_advantage(query_results)\n",
    "\n",
    "        for data, query_result in zip(datas_batch, query_results):\n",
    "            query_id = data['query_id']\n",
    "\n",
    "            if query_id == query_result._query_id:\n",
    "                data['query_result'] = query_result.to_dict()\n",
    "\n",
    "                out_file_path = f'{out_dir}/{query_id}.json'\n",
    "                file_utils.make_parent(out_file_path)\n",
    "\n",
    "                with open(out_file_path, 'w', encoding='utf-8') as out_file:\n",
    "                    json.dump(data, out_file, ensure_ascii=False, indent=4)\n",
    "            else:\n",
    "                print(f'# [error] chain_generate_for_sft() query_id is diff : {query_id} - {query_result._query_id}')\n",
    "\n",
    "\n",
    "def merge_to_jsonl(in_dir, out_file_path):\n",
    "    in_file_paths = file_utils.get_file_paths(in_dir)\n",
    "    print(f'{in_dir} : {len(in_file_paths)} files merge')\n",
    "\n",
    "    file_utils.make_parent(out_file_path)\n",
    "    with open(out_file_path, 'w', encoding='utf-8') as out_file:\n",
    "        for in_file_path in in_file_paths:\n",
    "            with open(in_file_path, 'r', encoding='utf-8') as in_file:\n",
    "                data = json.load(in_file)\n",
    "                json_line = json.dumps(data, ensure_ascii=False)\n",
    "                out_file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chains, chain_depth = 32, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df501ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = f'{out_dir}/train_5000_n_chains-{n_chains}_chain_depth-{chain_depth}'\n",
    "\n",
    "chain_generate_for_sft(train_datas, 100, n_chains, chain_depth, out_path)\n",
    "merge_to_jsonl(out_path, f'{out_path}_merged.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b4f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = f'{out_dir}/test_1000_n_chains-{n_chains}_chain_depth-{chain_depth}'\n",
    "\n",
    "chain_generate_for_sft(test_datas, 100, n_chains, chain_depth, out_path)\n",
    "merge_to_jsonl(out_path, f'{out_path}_merged.jsonl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ranger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
