DEBUG:
  LOG: true
  ERROR: true
  VLLM: true
  CORAG: true
  CORAG_SEARCH: false
  CORAG_TEST: false
  CHAIN: true
  CHAIN_TIME: true
  CHAIN_SERVER: true
  CHAIN_CLIENT: true
  TRAIN: true


# elasticsearch-7.10.1/config/elasticsearch.yml
ELASTICSEARCH_CONFIG:
  HOST: '127.0.0.1'
  PORT: 9200


# scripts/start_retriever_server.sh
RETRIEVER_SERVER_CONFIG:
  HOST: '127.0.0.1'
  PORT: 8200
  URL_RETRIEVE: '/retrieve'


VLLM_CONFIG:
  MODEL_NAME: 'meta-llama/Llama-3.2-3B-Instruct'
  DEVICE: 0
  DTYPE: 'float16'
  MAX_SEQ_LENGTH: 4096
  MAX_NEW_TOKENS: 128
  TEMPERATURE: 0.0
  GPU_MEMORY_UTILIZATION: 0.9
  N_LOG_PROB: 20 # 고정해도 무방


CORAG_CONFIG:
  TOP_K_QUERY: 20
  TOP_K_SUB_QUERY: 5
  TASK_DESC: 'answer multi-hop questions'


CHAIN_SERVER_CONFIG:
  HOST: '127.0.0.1'
  PORT: 7200
  DEBUG: false
  URL_GENERATE: '/chain/generate'
  URL_RESET: '/chain/reset'


MODEL_CONFIG:
  MODEL_NAME: 'meta-llama/Llama-3.2-3B-Instruct'
  DTYPE: 'float16'
  MAX_SEQ_LENGTH: 4096
  LORA_R: 8
  LORA_TARGET_MODULES: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
  LORA_ALPHA: 8
  GRADIENT_ACCUMULATION_STEPS: 16
  LEARNING_RATE: 0.0001 # '0.00005 ~ 0.0002' 튜닝 필요
  EPSILON: 0.2 # '0.2'로 고정해도 무방 (모델이 한 번의 업데이트에서 너무 급격하게 변하지 않도록 안전 장치를 거는 비율)
  KL_PENALTY: 0.01 # '0.01 ~ 0.05' (참조 모델과 너무 달라지지 않도록 하는 규제)
  USE_GRADIENT_CHECKPOINTING: false
  RESUME_RUN_TIME: false # (Ex. '2025-12-03-07-13-16')


REWARD_CONFIG:
  REWARD_OPTION: 7 # CORRECT:1, LENGTH:2, CORRECT+LENGTH:3 CONFIDENCE:4, ... ALL:7

