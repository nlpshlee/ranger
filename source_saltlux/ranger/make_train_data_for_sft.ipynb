{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1564e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from _init import *\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random, json\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ranger.utils import common_utils, json_utils, tokenizer_utils, file_utils, container_utils\n",
    "from ranger.corag.corag_result import ChainResult, QueryResult\n",
    "from ranger.corag import corag_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c2df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "common_utils.set_seed(seed)\n",
    "\n",
    "work_dir = f'/home/nlpshlee/dev_env/git/repos/ranger'\n",
    "data_dir = f'{work_dir}/data'\n",
    "sft_dir = f'{data_dir}/sft'\n",
    "selected_dir = f'{sft_dir}/selected'\n",
    "train_dir = f'{sft_dir}/selected_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e08060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_graph(data, xlabel, ylabel, title):\n",
    "#     sorted_items = sorted(data.items(), key=lambda x: int(x[0]))\n",
    "\n",
    "#     keys = [item[0] for item in sorted_items]\n",
    "#     values = [item[1] for item in sorted_items]\n",
    "\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     bars = plt.bar(keys, values, color='skyblue')\n",
    "\n",
    "#     for bar in bars:\n",
    "#         yval = bar.get_height()\n",
    "#         # x: 막대 중심, y: 막대 높이 + 5(여백), s: 값 텍스트\n",
    "#         plt.text(bar.get_x() + bar.get_width()/2, yval + 5, int(yval), ha='center', va='bottom')\n",
    "\n",
    "#     plt.xlabel(xlabel)\n",
    "#     plt.ylabel(ylabel)\n",
    "#     plt.title(title)\n",
    "#     # plt.savefig('dict_bar_chart_with_values.png')\n",
    "\n",
    "\n",
    "def plot_graph(data, xlabel, ylabel, title):\n",
    "    # 1. 키를 정수로 변환하여 정렬\n",
    "    sorted_items = sorted(data.items(), key=lambda x: float(x[0]))\n",
    "\n",
    "    keys = [item[0] for item in sorted_items]\n",
    "    values = [item[1] for item in sorted_items]\n",
    "\n",
    "    # 2. 전체 합계 계산 (비율 계산용)\n",
    "    total_sum = sum(values)\n",
    "\n",
    "    plt.figure(figsize=(12, 6)) # 텍스트가 길어지므로 가로를 좀 더 넓게 잡는 것을 추천합니다.\n",
    "    bars = plt.bar(keys, values, color='skyblue')\n",
    "\n",
    "    # 3. 누적 합계를 계산하며 텍스트 표기\n",
    "    cum_value = 0\n",
    "    \n",
    "    # zip을 사용하여 막대 객체와 값을 동시에 순회\n",
    "    for bar, value in zip(bars, values):\n",
    "        cum_value += value # 누적 합계 업데이트\n",
    "        \n",
    "        # 비율 계산\n",
    "        percent = (value / total_sum) * 100\n",
    "        cum_percent = (cum_value / total_sum) * 100\n",
    "        \n",
    "        # 텍스트 포맷: 값 (개별% / 누적%)\n",
    "        # 예: 232 (13.10% / 90.00%)\n",
    "        label_text = f\"{int(value)}({percent:.2f}%) / {int(cum_value)}({cum_percent:.2f}%)\"\n",
    "        \n",
    "        yval = bar.get_height()\n",
    "        \n",
    "        # 텍스트 출력 (글자가 겹치지 않게 fontsize를 조정하거나 rotation을 줄 수 있음)\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, \n",
    "                 yval + (max(values) * 0.01), # 막대 높이에 따라 유동적으로 위쪽 여백 조정\n",
    "                 label_text, \n",
    "                 ha='center', \n",
    "                 va='bottom',\n",
    "                 fontsize=9) # 텍스트가 길어서 폰트 사이즈를 조금 줄임\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    \n",
    "    # 그래프 여백 자동 조정 (텍스트 잘림 방지)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig('custom_bar_chart.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27289eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_is_stop(in_dir):\n",
    "    in_file_paths = file_utils.get_file_paths(in_dir)\n",
    "    print(f'{in_dir} size : {len(in_file_paths)}')\n",
    "\n",
    "    all_cnt_is_stop = 0\n",
    "    sum_cnt_is_stop = 0\n",
    "    chain_depth_cnt_dict = {}\n",
    "    f1_chain_depth_cnt_dict = {}\n",
    "    chain_num_cnt_dict = {}\n",
    "    min_chain_depth_cnt_dict = {}\n",
    "\n",
    "    all_cnt_f1 = 0\n",
    "    sum_cnt_f1 = 0\n",
    "    f1_cnt_dict = {}\n",
    "\n",
    "    for in_file_path in in_file_paths:\n",
    "        with open(in_file_path, 'r', encoding='utf-8') as in_file:\n",
    "            data = json.load(in_file)\n",
    "            query_result = data['query_result']\n",
    "            chain_results = query_result['chain_results']\n",
    "\n",
    "            cnt_is_stop = 0\n",
    "            min_chain_depth = 10000\n",
    "\n",
    "            for chain_result in chain_results:\n",
    "                is_stop = chain_result['is_stop']\n",
    "\n",
    "                if bool(is_stop):\n",
    "                    cnt_is_stop += 1\n",
    "\n",
    "                    chain_depth = len(chain_result['final_answers'])\n",
    "                    container_utils.add_str_int(chain_depth_cnt_dict, str(chain_depth), 1)\n",
    "\n",
    "                    if chain_depth < min_chain_depth:\n",
    "                        min_chain_depth = chain_depth\n",
    "            \n",
    "            if cnt_is_stop > 0:\n",
    "                all_cnt_is_stop += 1\n",
    "                sum_cnt_is_stop += cnt_is_stop\n",
    "                container_utils.add_str_int(chain_num_cnt_dict, str(cnt_is_stop), 1)\n",
    "                container_utils.add_str_int(min_chain_depth_cnt_dict, str(min_chain_depth), 1)\n",
    "            else:\n",
    "                cnt_f1 = 0\n",
    "\n",
    "                for chain_result in chain_results:\n",
    "                    f1 = float(chain_result['f1'])\n",
    "\n",
    "                    # if f1 == 1.0:\n",
    "                    #     print(f'in_file_path : {in_file_path}')\n",
    "                    #     sys.exit(-1)\n",
    "\n",
    "                    if f1 > 0.0:\n",
    "                        cnt_f1 += 1\n",
    "                        container_utils.add_str_int(f1_cnt_dict, f'{f1:.1f}', 1)\n",
    "\n",
    "                        chain_depth = len(chain_result['final_answers'])\n",
    "                        container_utils.add_str_int(f1_chain_depth_cnt_dict, str(chain_depth), 1)\n",
    "                \n",
    "                if cnt_f1 > 0:\n",
    "                    all_cnt_f1 += 1\n",
    "                    sum_cnt_f1 += cnt_f1\n",
    "\n",
    "    \n",
    "    print(f'all_cnt_is_stop : {all_cnt_is_stop}')\n",
    "    print(f'sum_cnt_is_stop : {sum_cnt_is_stop}')\n",
    "    print(f'chain_depth_cnt_dict : {sum(chain_depth_cnt_dict.values())} : {container_utils.sorted_dict_key(chain_depth_cnt_dict)}')\n",
    "    print(f'min_chain_depth_cnt_dict : {sum(min_chain_depth_cnt_dict.values())} : {container_utils.sorted_dict_key(min_chain_depth_cnt_dict)}')\n",
    "    print(f'chain_num_cnt_dict : {sum(chain_num_cnt_dict.values())} : {container_utils.sorted_dict_key(chain_num_cnt_dict)}\\n')\n",
    "\n",
    "    print(f'all_cnt_f1 : {all_cnt_f1}')\n",
    "    print(f'sum_cnt_f1 : {sum_cnt_f1}')\n",
    "    print(f'f1_cnt_dict : {sum(f1_cnt_dict.values())} : {container_utils.sorted_dict_key(f1_cnt_dict)}')\n",
    "    print(f'f1_chain_depth_cnt_dict : {sum(f1_chain_depth_cnt_dict.values())} : {container_utils.sorted_dict_key(f1_chain_depth_cnt_dict)}\\n')\n",
    "\n",
    "    print(f'all_cnt : {all_cnt_is_stop+all_cnt_f1}')\n",
    "    print(f'sum_cnt : {sum_cnt_is_stop+sum_cnt_f1}\\n')\n",
    "\n",
    "\n",
    "    plot_graph(\n",
    "        chain_depth_cnt_dict,\n",
    "        'chain depth',\n",
    "        'count',\n",
    "        ''\n",
    "    )\n",
    "\n",
    "    plot_graph(\n",
    "        min_chain_depth_cnt_dict,\n",
    "        'min chain depth per query',\n",
    "        'count',\n",
    "        ''\n",
    "    )\n",
    "\n",
    "    plot_graph(\n",
    "        chain_num_cnt_dict,\n",
    "        'number of chain(is_stop) per query',\n",
    "        'count',\n",
    "        ''\n",
    "    )\n",
    "\n",
    "    plot_graph(\n",
    "        f1_cnt_dict,\n",
    "        'f1-score',\n",
    "        'count',\n",
    "        ''\n",
    "    )\n",
    "\n",
    "    # plot_graph(\n",
    "    #     f1_chain_depth_cnt_dict,\n",
    "    #     'chain depth (f1)',\n",
    "    #     'count',\n",
    "    #     ''\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b3942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_dir = f'{sft_dir}/train_5000_n_chains-5_chain_depth-5'\n",
    "# check_is_stop(in_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a442488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_dir = f'{sft_dir}/train_5000_n_chains-32_chain_depth-10'\n",
    "# check_is_stop(in_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_candidate(in_dir, max_depth, max_chains, out_dir):\n",
    "    in_file_paths = file_utils.get_file_paths(in_dir)\n",
    "    print(f'{in_dir} size : {len(in_file_paths)}')\n",
    "\n",
    "    all_cnt_is_stop = 0\n",
    "    sum_cnt_is_stop = 0\n",
    "    \n",
    "    all_cnt_selected = 0\n",
    "    sum_cnt_selected = 0\n",
    "\n",
    "    for in_file_path in in_file_paths:\n",
    "        # in_file_path = '/home/nlpshlee/dev_env/git/repos/ranger/data/sft/train_5000_n_chains-32_chain_depth-10/2hop__720223_10690.json'\n",
    "\n",
    "        with open(in_file_path, 'r', encoding='utf-8') as in_file:\n",
    "            data = json.load(in_file)\n",
    "            query_result = data['query_result']\n",
    "            chain_results = query_result['chain_results']\n",
    "\n",
    "            cnt_is_stop = 0\n",
    "            chain_depth_dict = {}\n",
    "\n",
    "            for _chain_result in chain_results:\n",
    "                chain_result: ChainResult = ChainResult.from_dict(_chain_result)\n",
    "\n",
    "                chain_depth = len(chain_result._final_answers)\n",
    "                # print(f'chain_depth : {chain_depth}')\n",
    "\n",
    "                if chain_result._is_stop and chain_depth <= max_depth:\n",
    "                    cnt_is_stop += 1\n",
    "                    \n",
    "                    if chain_depth in chain_depth_dict.keys():\n",
    "                        chain_depth_dict[chain_depth].append(chain_result)\n",
    "                    else:\n",
    "                        chain_depth_dict[chain_depth] = [chain_result]\n",
    "\n",
    "            if 0 < cnt_is_stop:\n",
    "                all_cnt_is_stop += 1\n",
    "                sum_cnt_is_stop += cnt_is_stop\n",
    "            \n",
    "            concated_sq_set = set()\n",
    "            selected_crs = []\n",
    "\n",
    "            for chain_depth in container_utils.sorted_dict_key(chain_depth_dict).keys():\n",
    "                chain_results = chain_depth_dict[chain_depth]\n",
    "                # print(f'chain_depth : {chain_depth}, len : {len(chain_results)}')\n",
    "\n",
    "                for i in range(len(chain_results)):\n",
    "                    chain_result: ChainResult = chain_results[i]\n",
    "                    concated_sq = '#%#'.join(chain_result._sub_querys)\n",
    "\n",
    "                    if not concated_sq in concated_sq_set:\n",
    "                        # print(f'concated_sq : {concated_sq}')\n",
    "\n",
    "                        selected_crs.append(chain_result)\n",
    "                        concated_sq_set.add(concated_sq)\n",
    "\n",
    "                        if len(selected_crs) == max_chains:\n",
    "                            break\n",
    "                \n",
    "                if len(selected_crs) == max_chains:\n",
    "                    break\n",
    "            \n",
    "            if 0 < len(selected_crs):\n",
    "                all_cnt_selected += 1\n",
    "                sum_cnt_selected += len(selected_crs)\n",
    "\n",
    "                selected_crs_dict = [chain_result.to_dict() for chain_result in selected_crs]\n",
    "                data['query_result']['chain_results'] = selected_crs_dict\n",
    "\n",
    "                out_file_path = f'{out_dir}/{file_utils.get_file_name(in_file_path)}'\n",
    "                file_utils.make_parent(out_file_path)\n",
    "\n",
    "                with open(out_file_path, 'w', encoding='utf-8') as out_file:\n",
    "                    json.dump(data, out_file, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        # break\n",
    "\n",
    "    print(f'all_cnt_is_stop : {all_cnt_is_stop}')\n",
    "    print(f'sum_cnt_is_stop : {sum_cnt_is_stop}')\n",
    "    print(f'all_cnt_selected : {all_cnt_selected}')\n",
    "    print(f'sum_cnt_selected : {sum_cnt_selected}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d05e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_dir = 'train_5000_n_chains-32_chain_depth-10'\n",
    "# max_depth, max_chains = 5, 5\n",
    "\n",
    "# in_dir = f'{sft_dir}/{target_dir}'\n",
    "# out_dir = f'{selected_dir}/{target_dir}_max_depth-{max_depth}_max_chains-{max_chains}'\n",
    "# select_candidate(in_dir, max_depth, max_chains, out_dir)\n",
    "# check_is_stop(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc650e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_train_data(train_datas: list, query_id, query, prompt, generated_text, is_final_answer, is_last):\n",
    "    if is_final_answer:\n",
    "        prompt['content'] = corag_prompts.add_decide_stop_or_continue_prompt(prompt['content'])\n",
    "\n",
    "        if not is_last:\n",
    "            generated_text = f'<CONTINUE> {generated_text}'\n",
    "        else:\n",
    "            generated_text = f'<STOP> {generated_text}'\n",
    "\n",
    "    train_data = {\n",
    "        'query_id': query_id,\n",
    "        'query': query,\n",
    "        'source': prompt,\n",
    "        'target': generated_text\n",
    "    }\n",
    "\n",
    "    train_datas.append(train_data)\n",
    "\n",
    "\n",
    "def make_train(in_dir, out_dir, merge_file_path):\n",
    "    file_utils.make_parent(merge_file_path)\n",
    "    with open(merge_file_path, 'w', encoding='utf-8') as merge_file:\n",
    "        in_file_paths = file_utils.get_file_paths(in_dir)\n",
    "        print(f'{in_dir} size : {len(in_file_paths)}')\n",
    "\n",
    "        merged_cnt = 0\n",
    "        for in_file_path in in_file_paths:\n",
    "            with open(in_file_path, 'r', encoding='utf-8') as in_file:\n",
    "                data = json.load(in_file)\n",
    "                query_id = data['query_id']\n",
    "                query = data['query']\n",
    "\n",
    "                query_result = data['query_result']\n",
    "                chain_results = query_result['chain_results']\n",
    "\n",
    "                train_datas = []\n",
    "\n",
    "                for _chain_result in chain_results:\n",
    "                    chain_result: ChainResult = ChainResult.from_dict(_chain_result)\n",
    "                    chain_depth = len(chain_result._final_answers)\n",
    "                    # print(f'chain_depth : {chain_depth}')\n",
    "\n",
    "                    # depth 별로 모든 prompt 는 list 로 되어 있지만, size 는 '1'\n",
    "                    for i in range(chain_depth):\n",
    "                        prompt = chain_result._sub_query_prompts[i][0]\n",
    "                        generated_text = chain_result._sub_querys[i]\n",
    "                        add_train_data(train_datas, query_id, query, prompt, generated_text, False, False)\n",
    "\n",
    "                        prompt = chain_result._sub_answer_prompts[i][0]\n",
    "                        generated_text = chain_result._sub_answers[i]\n",
    "                        add_train_data(train_datas, query_id, query, prompt, generated_text, False, False)\n",
    "\n",
    "                        prompt = chain_result._final_answer_prompts[i][0]\n",
    "                        generated_text = chain_result._final_answers[i]\n",
    "\n",
    "                        if i < chain_depth-1:\n",
    "                            add_train_data(train_datas, query_id, query, prompt, generated_text, True, False)\n",
    "                        else:\n",
    "                            add_train_data(train_datas, query_id, query, prompt, generated_text, True, True)\n",
    "\n",
    "                out_file_path = f'{out_dir}/{query_id}.json'\n",
    "                file_utils.make_parent(out_file_path)\n",
    "\n",
    "                with open(out_file_path, 'w', encoding='utf-8') as out_file:\n",
    "                    json.dump(train_datas, out_file, ensure_ascii=False, indent=4)\n",
    "                \n",
    "                for train_data in train_datas:\n",
    "                    json_line = json.dumps(train_data, ensure_ascii=False)\n",
    "                    merge_file.write(json_line + '\\n')\n",
    "                \n",
    "                merged_cnt += len(train_datas)\n",
    "    \n",
    "    print(f'{merge_file_path} size : {merged_cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a85b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = 'train_5000_n_chains-32_chain_depth-10_max_depth-5_max_chains-5'\n",
    "in_dir = f'{selected_dir}/{target_dir}'\n",
    "out_dir = f'{train_dir}/{target_dir}'\n",
    "merge_file_path = f'{train_dir}/{target_dir}_merged.jsonl'\n",
    "make_train(in_dir, out_dir, merge_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ranger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
